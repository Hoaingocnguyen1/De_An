{
  "question": "How self-attention works?",
  "description": "Methodology inquiry",
  "answer": "Self-attention is a mechanism that allows a model to understand the relationships between different words within a sentence by enabling words to \"pay attention to themselves\" (Source 5). This helps the model recognize how words relate to each other, grasp sentence structure, and distinguish references, such as identifying when multiple instances of \"tôi\" (I) refer to the same subject (Source 5).\n\nThe process of self-attention calculation involves three key vectors for each word:\n1.  **Query (q):** Represents what the word is looking for.\n2.  **Key (k):** Represents what the word offers.\n3.  **Value (v):** Represents the actual content or information of the word (Source 1).\n\n**Analogy:** Imagine searching on Google. Your search term is the query (q), the list of results that appear are the keys (k), and the content within those search results is the value (v) (Source 1).\n\n**How it works (partial explanation from sources):**\n\n1.  **Vector Generation:** The q, k, and v vectors are generated from the input words. Importantly, the same set of weights is reused to calculate the query vectors for all input words, regardless of the number of words. Similarly, the sets of weights for calculating key and value vectors are also reused for each input word (Source 2).\n2.  **Positional Information:** Before these calculations, words might incorporate positional information. For example, position values for words can be derived from corresponding y-axis coordinates on \"squiggles\" that align with the x-axis coordinate for each word (Source 4). This suggests that input embeddings are enhanced with position before being used to derive q, k, and v.\n3.  **Calculation Steps (hinted):** The sources indicate that a form of mathematical operation takes place. For instance, \"products\" are calculated and then added together to produce new values. This process can transform initial \"position encoded values\" representing a word into new, potentially richer, values for that word (Source 3).\n\n**Limitations and Missing Information:**\nWhile the sources introduce the core concepts of q, k, v and the purpose of self-attention, they do not fully detail the complete mathematical process. Specifically, the sources do not explain the standard self-attention calculation steps, which typically involve:\n*   Taking the dot product of the query (q) with all key (k) vectors to get raw attention scores.\n*   Scaling these scores.\n*   Applying a softmax function to turn these scores into probabilities (attention weights).\n*   Multiplying these attention weights by the value (v) vectors and summing them up to get the final output for each word.\nSource 3 mentions calculating \"products\" and adding them, but it doesn't lay out the full sequence of operations. Additionally, the question raised in Source 3 about why new values are generated instead of using the initial ones is not answered within the provided text.",
  "sources": [
    {
      "id": 1,
      "ku_id": "website_68fed22decb803c197cdfc3d_text_9",
      "source_uri": "https://viblo.asia/p/self-attention-va-multi-head-sefl-attention-trong-transformers-n1j4lO2aVwl",
      "ku_type": "text_chunk",
      "score": 0.8467013835906982,
      "rerank_score": "[Type: text_chunk] Bây giờ, chúng ta sẽ cùng đi sâu vào cách self-attention hoạt động. Đầu tiên, hãy xem cách tính self-attention bằng cách sử dụng các vector, sau đó sẽ chuyển sang cách cài đặt thực tế với các ma trận. Trước tiên, chúng ta cần làm rõ ba vector q (query), k (key), và v (value) là gì.\nGiống như tên gọi của chúng, hãy tưởng tượng bạn tìm kiếm một từ khóa trên Google. Từ khóa mà bạn nhập vào được xem là q (query), các kết quả xuất hiện là k (key), và nội dung trong các kết quả đó là v (value). Để tìm ra kết quả khớp nhất, ta cần đo lường mức độ tương đồng giữa q và k. Để thực hiện điều này, self-attention sử dụng tích vô hướng giữa các vector.",
      "preview": "Bây giờ, chúng ta sẽ cùng đi sâu vào cách self-attention hoạt động. Đầu tiên, hãy xem cách tính self-attention bằng cách sử dụng các vector, sau đó sẽ..."
    },
    {
      "id": 2,
      "ku_id": "youtube_68fed231ecb803c197cdfc53_segment_4",
      "source_uri": "https://youtu.be/zxQyTK8quyY?si=3V5hXmhV0DXL6a8r",
      "ku_type": "text_chunk",
      "score": 0.822508692741394,
      "rerank_score": "[Type: text_chunk] Note, before we move on, I want to point out a few details about self-attention. First, the weights that we use to calculate the self-attention queries are the exact same for lets and go. In other words, this example uses one set of weights for calculating self-attention queries regardless of how many words are in the input. Likewise, we reuse the sets of weights for calculating self-attention keys and values for each input word. This means that no matter how many words are input into the transformer, we just reuse the same sets of weights for self-attention queries, keys and values. The other thing I want to point out is that we can calculate the queries, keys and values for each word at the same time. In other words, we don't have to calculate the query, key and value for the first word first before moving on to the second word. And because we can do all of the computation at the same time, transformers can take advantage of parallel computing and run fast. Now that we understand the details of how self-attention works, let's shrink it down so we can keep building our transformer. Bam! Josh, you forgot something. If we want two values to represent lets, why don't we just use the two position encoded values we started with? First, the new self-attention values for each word contain input from all of the other words, and this helps give each word context. And this can help establish how each word in the input is related to the others. Also, if we think of this unit with its weights for calculating queries, keys and values as a self-attention cell, then in order to correctly establish how words are related in complicated sentences and paragraphs, we can create a stack of self-attention cells each with its own sets of weights that we apply to the position encoded values for each word to capture different relationships among the words. In the manuscript that first described transformers, they stacked eight self-attention cells, and they called this multi-head attention. Why eight instead of twelve or sixteen? I have no idea. Bam! Okay, going back to our simple example with only one self-attention cell, there's one more thing we need to do to encode the input. We take the position encoded values and add them to the self-attention values. These bypasses are called residual connections, and they make it easier to train complex neural networks by allowing the self-attention layer to establish relationships among the input words without having to also preserve the word embedding and position encoding information. Bam! And that's all we need to do to encode the input for this simple transformer. Double Bam! Note, this simple transformer only contains the parts required for encoding the input. Word embedding, position encoding, self-attention, and residual connections. These four features allow the transformer to encode words into numbers, encode the positions of the words, encode the relationships among the words, and relatively easily and quickly train in parallel. That said, there are lots of extra things we can add to a transformer, and we'll talk about those at the end of this quest. Bam! So, now that we've encoded the English input phrase, let's go, it's time to decode it into Spanish. In other words, the first part of a transformer is called an encoder, and now it's time to create the second part, a decoder. The decoder, just like the encoder, starts with word embedding. However, this time we create embedding values for the output vocabulary, which consists of the Spanish words, ear, vamos, e, and the EOS end of sequence token. Now, because we just finish encoding the English sentence, let's go, the decoder starts with embedding values for the EOS token. In this case, we're using the EOS token to start the decoding because that is a common way to initialize the process of decoding the encoded input sentence. However, sometimes you'll see people use SOS for start-of-sentence or start-of-sequence to initialize the process. Josh, starting with SOS makes more sense to me. Then you can do it that way, Squatch. I'm just saying a lot of people start with EOS.",
      "timestamp": "19:53",
      "preview": "Note, before we move on, I want to point out a few details about self-attention. First, the weights that we use to calculate the self-attention querie..."
    },
    {
      "id": 3,
      "ku_id": "youtube_68fed231ecb803c197cdfc53_segment_3",
      "source_uri": "https://youtu.be/zxQyTK8quyY?si=3V5hXmhV0DXL6a8r",
      "ku_type": "text_chunk",
      "score": 0.8266882300376892,
      "rerank_score": "[Type: text_chunk] And we add those products together to get negative 1.0. Then we do the same thing with a different pair of weights to get 3.7. We do this twice because we started out with two position encoded values that represent the word lets. And after doing the math two times, we still have two values representing the word lets. Josh, I don't get it. If we want two values to represent lets, why don't we just use the two values we started with? That's a great question, Squatch, and we'll answer it in a little bit. GURR! Anyway, for now, just know that we have these two new values to represent the word lets. And in transformer terminology, we call them query values. And now that we have query values for the word lets, let's use them to calculate the similarity between itself and the word go. And we do this by creating two new values, just like we did for the query to represent the word lets. And we create two new values to represent the word go. Both sets of new values are called key values. And we use them to calculate similarities with the query for lets. One way to calculate similarities between the query and the keys is to calculate something called a dot product. For example, in order to calculate the dot product similarity between the query and key for lets, we simply multiply each pair of numbers together and then add the products to get 11.7. Likewise, we can calculate the dot product similarity between the query for lets and the key for go. By multiplying the pairs of numbers together and adding the products to get negative 2.6, the relatively large similarity value for lets relative to itself 11.7, compared to the relatively small value for lets relative to the word go negative 2.6, tells us that lets is much more similar to itself than it is to the word go. That said, if you remember the example where the word it could relate to pizza or oven, the word it should have a relatively large similarity value with respect to the word pizza since it refers to pizza and not oven. Note, there's a lot to be said about calculating similarities in this context in the dot product, so if you're interested, check out the quests. Anyway, since lets is much more similar to itself than it is to the word go, then we want lets to have more influence on its encoding than the word go. When we do this, by first running the similarity scores through something called a softmax function, the main idea of a softmax function is that it preserves the order of the input values from low to high and translates them into numbers between 0 and 1 that add up to 1. So we can think of the output of the softmax function as a way to determine what percentage of each input word we should use to encode the word lets. In this case, because lets is so much more similar to itself than the word go, we'll use 100% of the word lets to encode lets and 0% of the word go to encode the word lets. Note, there's a lot more to be said about the softmax function, so if you're interested, check out the quest. Anyway, because we want 100% of the word lets to encode lets, we create two more values that will cleverly call values to represent the word lets and scale the values that represent lets by 1.0. Then we create two values to represent the word go and scale those values by 0.0. Lastly, we add the scaled values together and these sums, which combine separate encodings for both input words, lets and go relative to their similarity to lets, are the self-attention values for lets. Bam! Now that we have self-attention values for the word lets, it's time to calculate them for the word go. The good news is that we don't need to recalculate the keys and values. Instead, all we need to do is create the query that represents the word go and do the math. By first calculating the similarity scores between the new query and the keys and then run the similarity scores through a softmax and then scale the values and then add them together and we end up with the self-attention values for go.",
      "timestamp": "14:54",
      "preview": "And we add those products together to get negative 1.0. Then we do the same thing with a different pair of weights to get 3.7. We do this twice becaus..."
    },
    {
      "id": 4,
      "ku_id": "youtube_68fed231ecb803c197cdfc53_segment_2",
      "source_uri": "https://youtu.be/zxQyTK8quyY?si=3V5hXmhV0DXL6a8r",
      "ku_type": "text_chunk",
      "score": 0.7936645746231079,
      "rerank_score": "[Type: text_chunk] us the position value for the third embedding value, which for the first word is zero. Lastly, the red squiggle gives us the position value for the fourth embedding, which for the first word is one. Thus, the position values for the first word come from the corresponding y-axis coordinates on the squiggles. Now, to get the position values for the second word, we simply use the y-axis coordinates on the squiggles that correspond to the x-axis coordinate for the second word. Lastly, to get the position values for the third word, we use the y-axis coordinates on the squiggles that correspond to the x-axis coordinate for the third word. Note, because the sine and cosine squiggles are repetitive, it's possible that two words might get the same position or y-axis values. For example, the second and third words both got negative 0.9 for the first position value. However, because the squiggles get wider for larger embedding positions, and the more embedding values we have than the wider the squiggles get, then even with our repeat value here and there, we end up with a unique sequence of position values for each word. Thus, each input word ends up with a unique sequence of position values. Now all we have to do is add the position values to the embedding values, and we end up with the word embeddings plus positional encoding for the whole sentence, squatch eats pizza. Yum! Note, if we reverse the order of the input words to be, pizza eats squatch, then the embeddings for the first and third words get swapped. But the positional values for the first, second, and third word stay the same. And when we add the positional values to the embeddings, we end up with new positional encoding for the first and third words. And the second word, since it didn't move, stays the same. Thus, positional encoding allows a transformer to keep track of word order. Bam! Now let's go back to our simple example, where we are just trying to translate the English sentence, let's go. And add position values to the word embeddings. The first embedding for the first word, let's, gets zero. And the second embedding gets one. And the first embedding for the second word, go, gets negative 0.9. And the second embedding gets 0.4. Now we just do the math to get the positional encoding for both words. Bam! Now, because we're going to need all the space we can get, let's consolidate the math in the diagram. And let the sine and cosine and plus symbols represent the positional encoding. And now that we know how to keep track of each word's position, let's talk about how a transformer keeps track of the relationships among words. For example, if the input sentence was this, the pizza came out of the oven and it tasted good. Then this word, it, could refer to pizza, or potentially it could refer to the word oven. Josh, I've heard of good tasting pizza, but never a good tasting oven. I know, Squatch, that's why it's important that the transformer correctly associates the word it with pizza. The good news is that transformers have something called self-attention, which is a mechanism to correctly associate the word it with the word pizza. In general terms, self-attention works by seeing how similar each word is to all of the words in the sentence, including itself. For example, self-attention calculates the similarity between the first word, the, and all of the words in the sentence, including itself. And self-attention calculates these similarities for every word in the sentence. Once the similarities are calculated, they are used to determine how the transformer encodes each word. For example, if you looked at a lot of sentences about pizza, and the word it was more commonly associated with pizza than oven, then the similarity score for pizza will cause it to have a larger impact on how the word it is encoded by the transformer. Bam! Now that we know the main ideas of how self-attention works, let's look at the details. So let's go back to our simple example, where we had just added positional encoding to the word's lets and go. The first thing we do is multiply the position encoded values for the word lets by a pair of weights.",
      "timestamp": "9:54",
      "preview": "us the position value for the third embedding value, which for the first word is zero. Lastly, the red squiggle gives us the position value for the fo..."
    },
    {
      "id": 5,
      "ku_id": "website_68fed22decb803c197cdfc3d_text_6",
      "source_uri": "https://viblo.asia/p/self-attention-va-multi-head-sefl-attention-trong-transformers-n1j4lO2aVwl",
      "ku_type": "text_chunk",
      "score": 0.8235440254211426,
      "rerank_score": "[Type: text_chunk] Xét câu \"Tôi cảm thấy tôi không được khoẻ.\" self-attention với khả năng \"chú ý đến chính nó\" giúp mô hình nhận ra sự liên quan giữa hai lần xuất hiện của từ \"tôi\" . Mô hình sẽ phân biệt được rằng cả hai lần \"tôi\" đều chỉ cùng một chủ thể, đồng thời cũng nắm bắt được cấu trúc câu. Hay trong câu \"Tôi đọc sách của tôi.\" trong câu này, từ \"tôi\" cũng xuất hiện hai lần và lại có ngữ nghĩa liên quan đến chính người nói. Nhưng khi từ \"Tôi\" tự \"chú ý\" đến chính nó thì mô hình sẽ hiểu đây là chủ ngữ và thực hiện hành động đọc nhằm phân biệt với từ \"tôi\" thứ 2 không phải là chủ ngữ. Lúc này việc tự \"chú ý\" đến nó giúp mô hình hiểu ngữ cảnh một cách chính xác. Ngoài ra nó còn giúp giữ lại ngữ nghĩa cho từ, việc tự chú ý cho phép mỗi từ giữ lại ngữ nghĩa riêng của nó. Ví dụ, từ \"tôi\" khi xuất hiện lần thứ hai sẽ \"nhớ\" lại rằng nó đại diện cho người sở hữu cuốn sách, trong khi vẫn hiểu rõ rằng \"Tôi\" đầu tiên là chủ thể thực hiện hành động đọc.",
      "preview": "Xét câu \"Tôi cảm thấy tôi không được khoẻ.\" self-attention với khả năng \"chú ý đến chính nó\" giúp mô hình nhận ra sự liên quan giữa hai lần xuất hiện ..."
    }
  ],
  "stats": {
    "initial_candidates": 20,
    "reranked_results": 5,
    "vector_search_model": "voyage-3",
    "rerank_model": "rerank-2.5",
    "synthesis_model": "GeminiClient"
  }
}