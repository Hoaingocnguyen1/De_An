{
  "question": "Transformer architecture?",
  "description": "Methodology inquiry",
  "answer": "The Transformer is a prominent architecture in machine learning, particularly in natural language processing (NLP) and image processing, recognized for its ability to process information in parallel and capture complex relationships between elements in a sequence (Source 5).\n\nThe overall architecture of the Transformer model is based on an encoder-decoder structure, both of which are comprised of stacked self-attention and fully connected layers. Both the encoder and decoder extensively utilize residual connections and layer normalization within their layers (Source 1, Source 2).\n\n**Encoder Architecture:**\nThe encoder consists of N=6 identical layers (Source 1). Each of these encoder layers contains two main sub-layers:\n1.  A multi-head self-attention mechanism.\n2.  A position-wise feed-forward network.\nResidual connections are applied around each of these sub-layers, followed by layer normalization (Source 1).\n\n**Decoder Architecture:**\nThe Transformer decoder is also composed of six identical layers (Source 2). Each decoder layer includes:\n1.  A masked multi-head self-attention sub-layer, which is designed to prevent the model from attending to future positions in the sequence (Source 2).\n2.  A multi-head attention sub-layer that specifically processes the output generated by the encoder (Source 2).\nSimilar to the encoder, residual connections and layer normalization are incorporated within each decoder layer (Source 2).\n\n**Attention Mechanisms:**\nThe core of the Transformer's functionality lies in its attention mechanism. An attention function is defined as mapping a query and a set of key-value pairs to an output, which is a weighted sum of the values (Source 2). A crucial detail for self-attention is that the same sets of weights are used for calculating queries, keys, and values for each input word, irrespective of the total number of words in the input (Source 3). In the context of encoder-decoder attention, values for each input word are calculated, scaled by softmax percentages, and then added together to form the encoder-decoder attention values (Source 4).\n\n**Summary of Key Components:**\n*   **Stacked Encoder and Decoder:** Both with multiple identical layers (Source 1, Source 2).\n*   **Multi-Head Self-Attention:** Present in both encoder and decoder layers (Source 1, Source 2).\n*   **Masked Self-Attention:** A specific type of self-attention used in the decoder to maintain autoregressive properties (Source 2).\n*   **Encoder-Decoder Attention:** A mechanism within the decoder that allows it to attend to the output of the encoder (Source 2, Source 4).\n*   **Position-wise Feed-Forward Networks:** Found in the encoder layers (Source 1).\n*   **Residual Connections and Layer Normalization:** Applied throughout both the encoder and decoder layers to aid training stability and performance (Source 1, Source 2).\n*   **Weight Sharing:** Weights for queries, keys, and values are reused across different input words in self-attention (Source 3).\n\n**Limitations:**\nThe provided sources offer a high-level description of the Transformer architecture and its components. However, they do not delve into the specific mathematical formulations of the attention function, the precise structure of the position-wise feed-forward networks, or the detailed implementation of multi-head attention beyond its general description. The significance of the \"Attention Is All You Need\" paper is highlighted, but its specific technical contributions are not elaborated upon (Source 5).",
  "sources": [
    {
      "id": 1,
      "ku_id": "pdf_68fed1f6ecb803c197cdfc39_text_chunk_0",
      "source_uri": "documents/1706.03762.pdf",
      "ku_type": "text_chunk",
      "score": 0.7689262628555298,
      "rerank_score": "[Type: text_chunk] The text describes the overall architecture of the Transformer model, which utilizes stacked self-attention and fully connected layers for its encoder and decoder. It details the encoder's structure, consisting of N=6 identical layers, each with a multi-head self-attention mechanism and a position-wise feed-forward network, incorporating residual connections and layer normalization.",
      "page": 1,
      "preview": "The text describes the overall architecture of the Transformer model, which utilizes stacked self-attention and fully connected layers for its encoder..."
    },
    {
      "id": 2,
      "ku_id": "pdf_68fed1f6ecb803c197cdfc39_text_chunk_1",
      "source_uri": "documents/1706.03762.pdf",
      "ku_type": "text_chunk",
      "score": 0.6583712100982666,
      "rerank_score": "[Type: text_chunk] The text details the architecture of a Transformer decoder, comprising six identical layers with residual connections and layer normalization. It includes a masked self-attention sub-layer to prevent attending to future positions and a multi-head attention sub-layer that processes the encoder's output. Additionally, it defines an attention function as mapping a query and key-value pairs to a weighted sum output.",
      "page": 1,
      "preview": "The text details the architecture of a Transformer decoder, comprising six identical layers with residual connections and layer normalization. It incl..."
    },
    {
      "id": 3,
      "ku_id": "youtube_68fed231ecb803c197cdfc53_segment_4",
      "source_uri": "https://youtu.be/zxQyTK8quyY?si=3V5hXmhV0DXL6a8r",
      "ku_type": "text_chunk",
      "score": 0.7438346147537231,
      "rerank_score": "[Type: text_chunk] Note, before we move on, I want to point out a few details about self-attention. First, the weights that we use to calculate the self-attention queries are the exact same for lets and go. In other words, this example uses one set of weights for calculating self-attention queries regardless of how many words are in the input. Likewise, we reuse the sets of weights for calculating self-attention keys and values for each input word. This means that no matter how many words are input into the transformer, we just reuse the same sets of weights for self-attention queries, keys and values. The other thing I want to point out is that we can calculate the queries, keys and values for each word at the same time. In other words, we don't have to calculate the query, key and value for the first word first before moving on to the second word. And because we can do all of the computation at the same time, transformers can take advantage of parallel computing and run fast. Now that we understand the details of how self-attention works, let's shrink it down so we can keep building our transformer. Bam! Josh, you forgot something. If we want two values to represent lets, why don't we just use the two position encoded values we started with? First, the new self-attention values for each word contain input from all of the other words, and this helps give each word context. And this can help establish how each word in the input is related to the others. Also, if we think of this unit with its weights for calculating queries, keys and values as a self-attention cell, then in order to correctly establish how words are related in complicated sentences and paragraphs, we can create a stack of self-attention cells each with its own sets of weights that we apply to the position encoded values for each word to capture different relationships among the words. In the manuscript that first described transformers, they stacked eight self-attention cells, and they called this multi-head attention. Why eight instead of twelve or sixteen? I have no idea. Bam! Okay, going back to our simple example with only one self-attention cell, there's one more thing we need to do to encode the input. We take the position encoded values and add them to the self-attention values. These bypasses are called residual connections, and they make it easier to train complex neural networks by allowing the self-attention layer to establish relationships among the input words without having to also preserve the word embedding and position encoding information. Bam! And that's all we need to do to encode the input for this simple transformer. Double Bam! Note, this simple transformer only contains the parts required for encoding the input. Word embedding, position encoding, self-attention, and residual connections. These four features allow the transformer to encode words into numbers, encode the positions of the words, encode the relationships among the words, and relatively easily and quickly train in parallel. That said, there are lots of extra things we can add to a transformer, and we'll talk about those at the end of this quest. Bam! So, now that we've encoded the English input phrase, let's go, it's time to decode it into Spanish. In other words, the first part of a transformer is called an encoder, and now it's time to create the second part, a decoder. The decoder, just like the encoder, starts with word embedding. However, this time we create embedding values for the output vocabulary, which consists of the Spanish words, ear, vamos, e, and the EOS end of sequence token. Now, because we just finish encoding the English sentence, let's go, the decoder starts with embedding values for the EOS token. In this case, we're using the EOS token to start the decoding because that is a common way to initialize the process of decoding the encoded input sentence. However, sometimes you'll see people use SOS for start-of-sentence or start-of-sequence to initialize the process. Josh, starting with SOS makes more sense to me. Then you can do it that way, Squatch. I'm just saying a lot of people start with EOS.",
      "timestamp": "19:53",
      "preview": "Note, before we move on, I want to point out a few details about self-attention. First, the weights that we use to calculate the self-attention querie..."
    },
    {
      "id": 4,
      "ku_id": "youtube_68fed231ecb803c197cdfc53_segment_6",
      "source_uri": "https://youtu.be/zxQyTK8quyY?si=3V5hXmhV0DXL6a8r",
      "ku_type": "text_chunk",
      "score": 0.7305241227149963,
      "rerank_score": "[Type: text_chunk] determines what should be the first translated word. Now that we know what percentage of each input word to use when determining what should be the first translated word, we calculate values for each input word. And then scale those values by the softmax percentages. And then add the pairs of scaled values together to get the encoder decoder attention values. Bam! Now to make room for the next step, let's consolidate the encoder decoder attention in our diagram. Note, the sets of weights that we use to calculate the queries, keys, and values for encoder decoder attention are different from the sets of weights we use for self-attention. However, just like for self-attention, the sets of weights are copied and reused for each word. This allows the transformer to be flexible with the length of the inputs and outputs. And also, we can stack encoder decoder attention just like we can stack self-attention to keep track of words in complicated phrases. Bam! Now we add another set of residual connections. That allow the encoder decoder attention to focus on the relationships between the output words and the input words without having to preserve the self-attention or word and position encoding that happened earlier. Then we consolidate the math in the diagram. Lastly, we need a way to take these two values that represent the EOS token and the decoder and select one of the four output tokens, EER, VAMOS, E or EOS. So we run these two values through a fully connected layer that has one input for each value that represents the current token. So in this case, we have two inputs. And one output for each token in the output vocabulary, which in this case means four outputs. Note, a fully connected layer is just a simple narrow network with weights, numbers we multiply the inputs by, and biases, numbers we add to the sums of the products. And when we do the math, we get four output values, which we run through a final soft max function to select the first output word, VAMOS. Bam! Note, VAMOS is the Spanish translation for Let's Go. Triple boom? No, not yet. So far, the translation is correct, but the decoder doesn't stop until it outputs an EOS token. So let's consolidate our diagrams and plug the translated word VAMOS into a copy of the decoder's embedding layer into the math. First, we get the word embeddings for VAMOS. Then we add the positional encoding. Now we calculate self-attention values for VAMOS using the exact same weights that we used for the EOS token. Now add the residual connections, and calculate the encoder decoder attention using the same sets of weights that we used for the EOS token. Now we add more residual connections. Lastly, we run the values that represent VAMOS through the same fully connected layer and softmax that we used for the EOS token. And the second output from the decoder is EOS, so we are done decoding. Triple boom! At long last, we've shown how a transformer can encode a simple input phrase, Let's Go, and decode the encoding into the translated phrase VAMOS. In summary, Transformers use word embedding to convert words into numbers, positional encoding to keep track of word order, self-attention to keep track of word relationships within the input and output phrases, encoder decoder attention to keep track of things between the input and output phrases to make sure that important words in the input are not lost in the translation. And residual connections to allow each subunit, like self-attention, to focus on solving just one part of the problem. Now that we understand the main ideas of how Transformers work, let's talk about a few extra things we can add to them. In this example, we kept things super simple. However, if we had larger vocabularies and the original Transformer had 37,000 tokens and longer input and output phrases. Then, in order to get their model to work, they had to normalize the values after every step. For example, they normalize the values after positional encoding and after self-attention in both the encoder and the decoder.",
      "timestamp": "29:52",
      "preview": "determines what should be the first translated word. Now that we know what percentage of each input word to use when determining what should be the fi..."
    },
    {
      "id": 5,
      "ku_id": "website_68fed22decb803c197cdfc3d_text_1",
      "source_uri": "https://viblo.asia/p/self-attention-va-multi-head-sefl-attention-trong-transformers-n1j4lO2aVwl",
      "ku_type": "text_chunk",
      "score": 0.7570962309837341,
      "rerank_score": "[Type: text_chunk] Transformers là gì? Liệu nó có gì đặc biệt mà trong machine learning người ta nhắc đến nó nhiều như thế? Trước thời điểm Google giới thiệu bài báo nổi tiếng Attention Is All You Need thì transformers là \"Người máy biến hình\" là \"Máy biến áp\". Sau khi bài báo được công bố thì transformers lại trở thành một trong những kiến trúc nổi bật trong lĩnh vực xử lý ngôn ngữ tự nhiên (NLP) và xử lý ảnh. Với khả năng xử lý song song và nắm bắt các mối quan hệ phức tạp giữa các từ trong câu, transformers đã giải quyết các hạn chế của mạng RNN và các biến thể (LSTM, GRU...). Xương sống của kiến trúc này chính là cơ chế self-attention giúp mô hình tập trung vào các thông tin quan trọng, hiệu quả hơn trong việc hiểu ngữ cảnh của từng từ trong câu và cho phép các transformers có bộ nhớ cực kỳ dài hạn. Có thể ví transformers như là 1 hộp đen, trong một ứng dụng dịch máy, nó sẽ \"ngậm\" vào một câu trong một ngôn ngữ và \"thổi\" ra bản dịch của nó trong một ngôn ngữ khác.\n1. Self-Attention là gì?",
      "preview": "Transformers là gì? Liệu nó có gì đặc biệt mà trong machine learning người ta nhắc đến nó nhiều như thế? Trước thời điểm Google giới thiệu bài báo nổi..."
    }
  ],
  "stats": {
    "initial_candidates": 20,
    "reranked_results": 5,
    "vector_search_model": "voyage-3",
    "rerank_model": "rerank-2.5",
    "synthesis_model": "GeminiClient"
  }
}